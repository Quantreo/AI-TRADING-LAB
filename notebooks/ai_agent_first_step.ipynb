{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Chapter 2: First AI Agent with LangChain\n",
    "\n",
    "## Objectives\n",
    "By the end of this notebook, you will:\n",
    "- Understand what a **Large Language Model (LLM)** is and how it generates text.\n",
    "- Know the difference between a raw **LLM** and an **AI Agent**.\n",
    "- Structure a **prompt** (system / user).\n",
    "- Configure a model and control its **temperature**.\n",
    "- Chain simple components into a **LangChain pipeline**.\n"
   ],
   "id": "e2abd5d9956011d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T08:07:10.107210Z",
     "start_time": "2025-11-06T08:07:10.102971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ===============================================================\n",
    "# 0. Basic setup\n",
    "# ===============================================================\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n"
   ],
   "id": "a0f3b463d8904799",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Connecting to Groq (Basic Setup)\n",
    "\n",
    "Before we use any model, we need to connect to Groq’s API.\n",
    "\n",
    "1. Create a `.env` file in your project root with `GROQ_API_KEY=your_api_key_here`\n",
    "2. Make sure this file is **ignored by Git** (`.gitignore`).\n",
    "3. Load your environment variables and test the connection below.\n"
   ],
   "id": "ba8481fd24928f14"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T08:44:57.796756Z",
     "start_time": "2025-11-06T08:44:57.793084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "309ea2281e4edaf0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T08:45:07.389302Z",
     "start_time": "2025-11-06T08:45:07.386990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if the key is loaded correctly\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"❌ GROQ_API_KEY not found. Please create your .env file.\")\n",
    "else:\n",
    "    print(\"✅ GROQ_API_KEY loaded successfully.\")"
   ],
   "id": "7bd73099dee3548c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GROQ_API_KEY loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T08:45:09.507444Z",
     "start_time": "2025-11-06T08:45:09.204043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7)\n",
    "\n",
    "# Test the connection\n",
    "response = llm.invoke(\"Say hello, I’m connected to Groq.\")\n",
    "print(\"Model response:\", response.content)"
   ],
   "id": "1ecefd8df0dbcaad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: Hello, I see you're connected to Groq, a company that provides AI acceleration solutions. How's your experience with Groq so far? Are you working on any exciting projects or exploring the potential of their technology?\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you see a text response above, your connection to Groq works correctly.\n",
    "We can now move on to understanding **model parameters**, such as `temperature` and `model size`,\n",
    "to control how the model behaves.\n"
   ],
   "id": "e90836f78ddad9b2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Temperature and Model Size\n",
    "\n",
    "Now that our connection works, let’s explore two key model parameters:\n",
    "- **Model size:** defines the capacity and speed of the model.\n",
    "  Larger models handle more context but are slower and more expensive.\n",
    "- **Temperature:** controls randomness and creativity in the generation.\n",
    "  Lower values → consistent outputs, higher values → more varied answers.\n",
    "\n",
    "Let’s see how these parameters affect the output.\n"
   ],
   "id": "b5af3e51565e1563"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T08:50:46.080240Z",
     "start_time": "2025-11-06T08:50:45.259063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We'll use the same model: Llama 3.1 8B Instant\n",
    "prompt = \"Give me a very short feature that I can use in my trading in one sentence\"\n",
    "\n",
    "# Compare two temperature settings\n",
    "for temp in [0.2, 0.8]:\n",
    "    llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=temp)\n",
    "    print(f\"\\n--- Temperature = {temp} ---\")\n",
    "    print(llm.invoke(prompt).content)"
   ],
   "id": "b1ed6eeaed0d3b4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature = 0.2 ---\n",
      "The \"Golden Cross\" feature, which occurs when a short-term moving average (e.g. 50-day) crosses above a long-term moving average (e.g. 200-day), can be a bullish signal indicating a potential long-term trend reversal.\n",
      "\n",
      "--- Temperature = 0.8 ---\n",
      "The \"Donchian Channel\" feature, which involves setting buy or sell signals when the price crosses above or below its highest high or lowest low over a specified period of time (typically 20 days), can be a useful technical indicator for identifying potential trading opportunities.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T08:50:56.523666Z",
     "start_time": "2025-11-06T08:50:55.656444Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We'll use the same model: Llama 3.1 8B Instant\n",
    "prompt = \"Give me a very short feature that I can use in my trading in one sentence\"\n",
    "\n",
    "# Compare two temperature settings\n",
    "for temp in [0.2, 0.8]:\n",
    "    llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=temp)\n",
    "    print(f\"\\n--- Temperature = {temp} ---\")\n",
    "    print(llm.invoke(prompt).content)"
   ],
   "id": "6e761c36ce5dea8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Temperature = 0.2 ---\n",
      "Using a \"stop-loss\" feature can help limit your potential losses by automatically selling a security when it falls to a certain price, thereby protecting your investment.\n",
      "\n",
      "--- Temperature = 0.8 ---\n",
      "Using a \"stop-loss\" feature, which automatically sells a stock when it falls to a certain price, can help limit potential losses and protect your investments.\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you can see, a higher temperature generates more diverse or unexpected answers,\n",
    "while a lower temperature gives more deterministic and stable results.\n",
    "\n",
    "In trading contexts, we often prefer **stability and reproducibility**,\n",
    "so you’ll see moderate values (around 0.4–0.6) used throughout this course.\n"
   ],
   "id": "f8ef6c8520798b83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Building a Structured Prompt\n",
    "\n",
    "So far, we’ve used free text prompts.\n",
    "Now, we’ll see how **structured prompts** can help the model stay consistent.\n",
    "\n",
    "LangChain uses two message types:\n",
    "- **System** → defines the role (context, behavior)\n",
    "- **User** → gives the actual task to perform\n",
    "\n"
   ],
   "id": "9aef876a0ca3615d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T09:02:47.395817Z",
     "start_time": "2025-11-06T09:02:46.261995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3.1. Free text prompt (unstructured)\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7)\n",
    "response = llm.invoke(\"Create a simple configuration for a trading bot.\")\n",
    "print(response.content)\n"
   ],
   "id": "54328b817000a694",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Trading Bot Configuration**\n",
      "================================\n",
      "\n",
      "Below is a simple configuration for a trading bot in Python. This configuration includes basic settings such as API keys, trading pairs, and risk management parameters.\n",
      "\n",
      "### Configuration File (`config.py`)\n",
      "```python\n",
      "import os\n",
      "\n",
      "class Config:\n",
      "    # API Keys\n",
      "    BINANCE_API_KEY = os.environ.get('BINANCE_API_KEY')\n",
      "    BINANCE_API_SECRET = os.environ.get('BINANCE_API_SECRET')\n",
      "\n",
      "    # Trading Pairs\n",
      "    PAIRS = [\n",
      "        ('BTCUSDT', 0.01),  # Bitcoin / USDT with a minimum trade size of 0.01\n",
      "        ('ETHUSDT', 0.001),  # Ethereum / USDT with a minimum trade size of 0.001\n",
      "        ('LTCUSDT', 0.001)  # Litecoin / USDT with a minimum trade size of 0.001\n",
      "    ]\n",
      "\n",
      "    # Risk Management\n",
      "    MAX_LEVERAGE = 2\n",
      "    STOP_LOSS_PCT = 0.05\n",
      "    TAKE_PROFIT_PCT = 0.05\n",
      "\n",
      "    # Logging\n",
      "    LOG_LEVEL = 'INFO'\n",
      "\n",
      "    # Other Settings\n",
      "    EXCHANGE = 'BINANCE'\n",
      "    INTERVAL = '1m'  # 1 minute interval\n",
      "    START_TIME = '08:00'  # Start trading at 8:00 AM UTC\n",
      "    END_TIME = '17:00'  # End trading at 5:00 PM UTC\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "*   The configuration file loads API keys from environment variables for secure storage.\n",
      "*   The `PAIRS` list contains trading pairs with their minimum trade sizes.\n",
      "*   Risk management parameters include maximum leverage, stop loss percentage, and take profit percentage.\n",
      "*   Logging level is set to `INFO` for production environments, but can be adjusted for development or debugging purposes.\n",
      "*   The exchange, interval, start time, and end time are set to specific values, but can be modified to suit individual trading strategies.\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "To use this configuration in a trading bot, you can import the `Config` class and access its attributes as needed. For example:\n",
      "```python\n",
      "import config\n",
      "\n",
      "# Access API keys\n",
      "api_key = config.BINANCE_API_KEY\n",
      "api_secret = config.BINANCE_API_SECRET\n",
      "\n",
      "# Get trading pairs and risk management parameters\n",
      "pairs = config.PAIRS\n",
      "max_leverage = config.MAX_LEVERAGE\n",
      "stop_loss_pct = config.STOP_LOSS_PCT\n",
      "take_profit_pct = config.TAKE_PROFIT_PCT\n",
      "```\n",
      "\n",
      "Note that this is a basic configuration and may need to be adapted to specific trading strategies or exchanges. Additionally, be sure to handle errors and exceptions properly in your trading bot to ensure robustness and reliability.\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T09:02:47.626883Z",
     "start_time": "2025-11-06T09:02:47.456429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3.2. Structured prompt (system + user)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a quant developer. Always answer in valid YAML.\"),\n",
    "    (\"user\", \"Create a configuration for a trading bot with two parameters: window_size=20 and feature='momentum'.\")\n",
    "])\n",
    "\n",
    "formatted = prompt.format_messages()\n",
    "response = llm.invoke(formatted)\n",
    "print(response.content)\n"
   ],
   "id": "64d1294a986a6c01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```yml\n",
      "trading_bot:\n",
      "  parameters:\n",
      "    window_size: 20\n",
      "    feature: momentum\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T09:03:25.587944Z",
     "start_time": "2025-11-06T09:03:25.169366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3.3. Structured prompt with JSON output\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a quant developer. Answer strictly in JSON format.\"),\n",
    "    (\"user\", \"Give me 3 trading features with a short description each.\")\n",
    "])\n",
    "\n",
    "formatted = prompt.format_messages()\n",
    "response = llm.invoke(formatted)\n",
    "print(response.content)\n"
   ],
   "id": "abd448365c412c9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  {\n",
      "    \"name\": \"Mean Reversion Strategy\",\n",
      "    \"description\": \"A trading strategy that involves buying an asset when its price falls below its historical mean and selling when it rises above, based on the assumption that the asset's price will revert to its mean over time.\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Trend Following Strategy\",\n",
      "    \"description\": \"A trading strategy that involves identifying and following the direction of a trend in an asset's price, with the goal of profiting from the continuation of the trend.\"\n",
      "  },\n",
      "  {\n",
      "    \"name\": \"Momentum Trading Strategy\",\n",
      "    \"description\": \"A trading strategy that involves buying an asset when its price is rising rapidly and selling when it is falling rapidly, based on the assumption that the asset's momentum will continue to drive its price higher or lower.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "A good prompt defines:\n",
    "1. The **role** (system)\n",
    "2. The **task** (user)\n",
    "3. The **expected output format**\n",
    "\n",
    "This combination makes your results predictable, testable, and ready to integrate in automated pipelines."
   ],
   "id": "719971a38c1d9be1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Chaining Logic with RunnableSequence\n",
    "\n",
    "An agent is often made of **several logical steps**.\n",
    "\n",
    "Instead of writing a long, monolithic process,\n",
    "we can use LangChain’s `RunnableSequence` to **chain functions together**.\n",
    "\n",
    "Each step:\n",
    "1. Receives an input\n",
    "2. Transforms it\n",
    "3. Passes it to the next step\n",
    "\n",
    "Let’s see a simple example.\n"
   ],
   "id": "ca20e91f8c218563"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Step functions\n",
    "# ---------------------------------------------------------------\n",
    "def clean_text(txt: str) -> str:\n",
    "    \"\"\"Remove extra spaces and lowercase the text.\"\"\"\n",
    "    return txt.strip().lower()\n",
    "\n",
    "def add_prefix(txt: str) -> str:\n",
    "    \"\"\"Add a prefix before the text.\"\"\"\n",
    "    return f\"Processed: {txt}\""
   ],
   "id": "d085ae3907686f5b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T09:13:41.544335Z",
     "start_time": "2025-11-06T09:13:41.540919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Build the RunnableSequence\n",
    "# ---------------------------------------------------------------\n",
    "pipeline = RunnableSequence(\n",
    "    first=RunnableLambda(clean_text),\n",
    "    middle=[],\n",
    "    last=RunnableLambda(add_prefix)\n",
    ")"
   ],
   "id": "d0b183233a5a1c6b",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-06T09:13:42.068524Z",
     "start_time": "2025-11-06T09:13:42.065884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Run the sequence\n",
    "# ---------------------------------------------------------------\n",
    "output = pipeline.invoke(\"   Hello Quantreo!   \")\n",
    "print(output)"
   ],
   "id": "f8f451611bf2dcc2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: hello quantreo!\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "559df18bbd8f987a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
